{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d6f8a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1737df22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BERT models\n",
    "model1 = SentenceTransformer('paraphrase-MiniLM-L3-v2')\n",
    "model2 = SentenceTransformer('all-distilroberta-v1')\n",
    "model3 = SentenceTransformer('multi-qa-distilbert-cos-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25b74d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(user_ques_1,user_ques_2):\n",
    "    # Stop word removal\n",
    "    nltk.download('stopwords')\n",
    "    stop = stopwords.words('english')\n",
    "    # Handle missing values by replacing NaN with an empty string\n",
    "    ques_1_nstop = ' '.join([word for word in user_ques_1.split() if word not in stop])\n",
    "    ques_2_nstop = ' '.join([word for word in user_ques_2.split() if word not in stop])\n",
    "\n",
    "    # Tokenize the questions\n",
    "    nltk.download('punkt')\n",
    "    tok_ques_1 = nltk.word_tokenize(ques_1_nstop)\n",
    "    tok_ques_2 = nltk.word_tokenize(ques_2_nstop)\n",
    "    # Convert obtained lists result from tokenizer to string data points\n",
    "    tok_ques_1_str = ' '.join(map(str, tok_ques_1))\n",
    "    tok_ques_2_str = ' '.join(map(str, tok_ques_2))\n",
    "    # Lemmatize data\n",
    "    nltk.download('wordnet')\n",
    "    w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    def lemmatize_text(text):\n",
    "        return ' '.join([lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)])\n",
    "\n",
    "    ques_1_lemm_str = lemmatize_text(tok_ques_1_str)\n",
    "    ques_2_lemm_str = lemmatize_text(tok_ques_2_str)\n",
    "    # Convert lemmatized data to lower case\n",
    "    ques_1_lemm_str = ques_1_lemm_str.lower()\n",
    "    ques_2_lemm_str = ques_2_lemm_str.lower()\n",
    "    # Text similarity scores obtained using 'paraphrase-MiniLM-L3-v2' BERT model\n",
    "    st = time.time()\n",
    "    embd1 = model1.encode(ques_1_lemm_str, convert_to_tensor=True)\n",
    "    embd2 = model1.encode(ques_2_lemm_str, convert_to_tensor=True)\n",
    "    cosine_scores1 = util.pytorch_cos_sim(embd1, embd2)\n",
    "    et = time.time()\n",
    "    elapsed_time = et - st\n",
    "#    print(\"Similarity Score using BERT model 'paraphrase-MiniLM-L3-v2':\", cosine_scores1.item())\n",
    "#   print('Execution time:', elapsed_time, 'seconds')\n",
    "    # Text similarity scores obtained using BERT model 'all-distilroberta-v1'\n",
    "    st = time.time()\n",
    "    embd1 = model2.encode(ques_1_lemm_str, convert_to_tensor=True)\n",
    "    embd2 = model2.encode(ques_2_lemm_str, convert_to_tensor=True)\n",
    "    cosine_scores2 = util.pytorch_cos_sim(embd1, embd2)\n",
    "    et = time.time()\n",
    "    elapsed_time = et - st\n",
    "#     print(\"Similarity Score using BERT model 'all-distilroberta-v1':\", cosine_scores2.item())\n",
    "#     print('Execution time:', elapsed_time, 'seconds')\n",
    "    # Text similarity scores obtained using BERT model 'multi-qa-distilbert-cos-v1'\n",
    "    st = time.time()\n",
    "    embd1 = model3.encode(ques_1_lemm_str, convert_to_tensor=True)\n",
    "    embd2 = model3.encode(ques_2_lemm_str, convert_to_tensor=True)\n",
    "    cosine_scores3 = util.pytorch_cos_sim(embd1, embd2)\n",
    "    et = time.time()\n",
    "    elapsed_time = et - st\n",
    "#     print(\"Similarity Score using BERT model 'multi-qa-distilbert-cos-v1':\", cosine_scores3.item())\n",
    "#     print('Execution time:', elapsed_time, 'seconds')\n",
    "    # Calculate average similarity score\n",
    "    avg_similarity_score = (cosine_scores1.item() + cosine_scores2.item() + cosine_scores3.item())/3\n",
    "    return avg_similarity_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d222d11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Question 1: test\n",
      "Enter Question 2: test\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\shric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Get user input for two questions\n",
    "user_ques_1 = input(\"Enter Question 1: \")\n",
    "user_ques_2 = input(\"Enter Question 2: \")\n",
    "print(model(user_ques_1,user_ques_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b24a8fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebc8fad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Stop word removal\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42108e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values by replacing NaN with an empty string\n",
    "ques_1_nstop = ' '.join([word for word in user_ques_1.split() if word not in stop])\n",
    "ques_2_nstop = ' '.join([word for word in user_ques_2.split() if word not in stop])\n",
    "\n",
    "# Tokenize the questions\n",
    "nltk.download('punkt')\n",
    "tok_ques_1 = nltk.word_tokenize(ques_1_nstop)\n",
    "tok_ques_2 = nltk.word_tokenize(ques_2_nstop)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7742559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert obtained lists result from tokenizer to string data points\n",
    "tok_ques_1_str = ' '.join(map(str, tok_ques_1))\n",
    "tok_ques_2_str = ' '.join(map(str, tok_ques_2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3566c149",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\shric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize data\n",
    "nltk.download('wordnet')\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b48bab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    return ' '.join([lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)])\n",
    "\n",
    "ques_1_lemm_str = lemmatize_text(tok_ques_1_str)\n",
    "ques_2_lemm_str = lemmatize_text(tok_ques_2_str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2c87a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lemmatized data to lower case\n",
    "ques_1_lemm_str = ques_1_lemm_str.lower()\n",
    "ques_2_lemm_str = ques_2_lemm_str.lower()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e662631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score using BERT model 'paraphrase-MiniLM-L3-v2': 0.935845673084259\n",
      "Execution time: 0.07124948501586914 seconds\n"
     ]
    }
   ],
   "source": [
    "# Text similarity scores obtained using 'paraphrase-MiniLM-L3-v2' BERT model\n",
    "st = time.time()\n",
    "embd1 = model1.encode(ques_1_lemm_str, convert_to_tensor=True)\n",
    "embd2 = model1.encode(ques_2_lemm_str, convert_to_tensor=True)\n",
    "cosine_scores1 = util.pytorch_cos_sim(embd1, embd2)\n",
    "et = time.time()\n",
    "elapsed_time = et - st\n",
    "print(\"Similarity Score using BERT model 'paraphrase-MiniLM-L3-v2':\", cosine_scores1.item())\n",
    "print('Execution time:', elapsed_time, 'seconds')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "777e8738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score using BERT model 'all-distilroberta-v1': 0.8731243014335632\n",
      "Execution time: 0.08671975135803223 seconds\n"
     ]
    }
   ],
   "source": [
    "# Text similarity scores obtained using BERT model 'all-distilroberta-v1'\n",
    "st = time.time()\n",
    "embd1 = model2.encode(ques_1_lemm_str, convert_to_tensor=True)\n",
    "embd2 = model2.encode(ques_2_lemm_str, convert_to_tensor=True)\n",
    "cosine_scores2 = util.pytorch_cos_sim(embd1, embd2)\n",
    "et = time.time()\n",
    "elapsed_time = et - st\n",
    "print(\"Similarity Score using BERT model 'all-distilroberta-v1':\", cosine_scores2.item())\n",
    "print('Execution time:', elapsed_time, 'seconds')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2f66da84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score using BERT model 'multi-qa-distilbert-cos-v1': 0.8531126976013184\n",
      "Execution time: 0.0839846134185791 seconds\n"
     ]
    }
   ],
   "source": [
    "# Text similarity scores obtained using BERT model 'multi-qa-distilbert-cos-v1'\n",
    "st = time.time()\n",
    "embd1 = model3.encode(ques_1_lemm_str, convert_to_tensor=True)\n",
    "embd2 = model3.encode(ques_2_lemm_str, convert_to_tensor=True)\n",
    "cosine_scores3 = util.pytorch_cos_sim(embd1, embd2)\n",
    "et = time.time()\n",
    "elapsed_time = et - st\n",
    "print(\"Similarity Score using BERT model 'multi-qa-distilbert-cos-v1':\", cosine_scores3.item())\n",
    "print('Execution time:', elapsed_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08e4f46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Similarity Score: 0.8873608907063802\n"
     ]
    }
   ],
   "source": [
    "# Calculate average similarity score\n",
    "avg_similarity_score = (cosine_scores1.item() + cosine_scores2.item() + cosine_scores3.item())/3\n",
    "print(\"Average Similarity Score:\", avg_similarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5e57c1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting reportlab\n",
      "  Downloading reportlab-4.0.6-py3-none-any.whl (1.9 MB)\n",
      "     ---------------------------------------- 1.9/1.9 MB 10.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\shric\\anaconda3\\lib\\site-packages (from reportlab) (9.4.0)\n",
      "Installing collected packages: reportlab\n",
      "Successfully installed reportlab-4.0.6\n"
     ]
    }
   ],
   "source": [
    "!pip install reportlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cfb952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "\n",
    "# Create a PDF document\n",
    "pdf_file = \"output.pdf\"\n",
    "c = canvas.Canvas(pdf_file, pagesize=letter)\n",
    "\n",
    "# Set font and other attributes\n",
    "c.setFont(\"Helvetica\", 12)\n",
    "line_height = 14  # Adjust line height as needed\n",
    "\n",
    "# Iterate through the list and add the content to the PDF\n",
    "y_position = letter[1] - 72  # Start position (72 points from the top)\n",
    "for content in content_list:\n",
    "    c.drawString(72, y_position, content)\n",
    "    y_position -= line_height\n",
    "\n",
    "# Save the PDF document\n",
    "c.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96525fea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
