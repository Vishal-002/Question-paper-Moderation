{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e83b47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import nltk\n",
    "import pdfplumber\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b560f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<flask_cors.extension.CORS at 0x1c5b3e22e00>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "CORS(app)  # Enable CORS for all routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3cffc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK data (if not already downloaded)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3523b846",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model(user_ques_1, user_ques_2):\n",
    "    # Stop word removal\n",
    "    stop = stopwords.words('english')\n",
    "    ques_1_nstop = ' '.join([word for word in user_ques_1.split() if word not in stop])\n",
    "    ques_2_nstop = ' '.join([word for word in user_ques_2.split() if word not in stop])\n",
    "\n",
    "    # Tokenize the questions\n",
    "    tok_ques_1 = nltk.word_tokenize(ques_1_nstop)\n",
    "    tok_ques_2 = nltk.word_tokenize(ques_2_nstop)\n",
    "    tok_ques_1_str = ' '.join(map(str, tok_ques_1))\n",
    "    tok_ques_2_str = ' '.join(map(str, tok_ques_2))\n",
    "\n",
    "    # Lemmatize data\n",
    "    w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "    def lemmatize_text(text):\n",
    "        return ' '.join([lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)])\n",
    "\n",
    "    ques_1_lemm_str = lemmatize_text(tok_ques_1_str)\n",
    "    ques_2_lemm_str = lemmatize_text(tok_ques_2_str)\n",
    "\n",
    "    # Convert lemmatized data to lower case\n",
    "    ques_1_lemm_str = ques_1_lemm_str.lower()\n",
    "    ques_2_lemm_str = ques_2_lemm_str.lower()\n",
    "\n",
    "    # Text similarity scores obtained using 'paraphrase-MiniLM-L3-v2' BERT model\n",
    "    st = time.time()\n",
    "    embd1 = model1.encode(ques_1_lemm_str, convert_to_tensor=True)\n",
    "    embd2 = model1.encode(ques_2_lemm_str, convert_to_tensor=True)\n",
    "    cosine_scores1 = util.pytorch_cos_sim(embd1, embd2)\n",
    "    et = time.time()\n",
    "    elapsed_time = et - st\n",
    "\n",
    "    # Text similarity scores obtained using BERT model 'all-distilroberta-v1'\n",
    "    st = time.time()\n",
    "    embd1 = model2.encode(ques_1_lemm_str, convert_to_tensor=True)\n",
    "    embd2 = model2.encode(ques_2_lemm_str, convert_to_tensor=True)\n",
    "    cosine_scores2 = util.pytorch_cos_sim(embd1, embd2)\n",
    "    et = time.time()\n",
    "    elapsed_time = et - st\n",
    "\n",
    "    # Text similarity scores obtained using BERT model 'multi-qa-distilbert-cos-v1'\n",
    "    st = time.time()\n",
    "    embd1 = model3.encode(ques_1_lemm_str, convert_to_tensor=True)\n",
    "    embd2 = model3.encode(ques_2_lemm_str, convert_to_tensor=True)\n",
    "    cosine_scores3 = util.pytorch_cos_sim(embd1, embd2)\n",
    "    et = time.time()\n",
    "    elapsed_time = et - st\n",
    "\n",
    "    # Calculate average similarity score\n",
    "    avg_similarity_score = (cosine_scores1.item() + cosine_scores2.item() + cosine_scores3.item()) / 3\n",
    "\n",
    "    return avg_similarity_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24f7203d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        text = \"\"\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25e095ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/process_pdfs', methods=['POST'])\n",
    "def process_pdfs():\n",
    "    try:\n",
    "        # Assuming you are sending the PDF files in the request\n",
    "        file1 = request.files['file1']\n",
    "        file2 = request.files['file2']\n",
    "\n",
    "        # Extract text from both PDF files\n",
    "        question_paper1_text = extract_text_from_pdf(file1)\n",
    "        question_paper2_text = extract_text_from_pdf(file2)\n",
    "\n",
    "        # Use regular expressions to split the text into individual questions\n",
    "        arr1 = re.split(r'\\d+\\.', question_paper1_text)\n",
    "        arr2 = re.split(r'\\d+\\.', question_paper2_text)\n",
    "\n",
    "        # Remove any leading or trailing whitespace from the questions\n",
    "        arr1 = [question.strip() for question in arr1 if question.strip()]\n",
    "        arr2 = [question.strip() for question in arr2 if question.strip()]\n",
    "\n",
    "        new_paper = []\n",
    "\n",
    "        for q1 in arr1:\n",
    "            for q2 in arr2:\n",
    "                similarity = model(q1, q2)\n",
    "                if similarity > 0.7:\n",
    "                    new_paper.append(q1)\n",
    "                    break  # Break to avoid adding duplicates\n",
    "\n",
    "        # Add questions from arr2 that didn't have matches\n",
    "        for q2 in arr2:\n",
    "            if q2 not in new_paper:\n",
    "                new_paper.append(q2)\n",
    "\n",
    "        # Add questions from arr1 that didn't have matches\n",
    "        for q1 in arr1:\n",
    "            if q1 not in new_paper:\n",
    "                new_paper.append(q1)\n",
    "\n",
    "        # Respond with the result\n",
    "        return jsonify({\"result\": new_paper})\n",
    "\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9cf8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True, port=5000, use_reloader=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d70d2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13685655",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
